# general
seed: 200
exp_id: 200
log_dir: "logs"

dataset_pt: "web-text-tok-llama2.hf"
split_pt: #Â none
dataset: "muse-news-train.hf"
split: "forget"
use_chunking: False

# change "news" to "books" everywhere to run for "books"
gen_eval_name: "news"
tokenizer: "muse-news-target"
network: "muse-news-target" 
network_base: "muse-news-target" 

# early stopping check
stop_metric: "verbmem_f"
stop_val: 22.0 # stops once metric 'stop_metric' is smaller or equal to 'stop_val'

context: 1024

loss: "npo"
penalty: "kl"

optimizer: "adamw"
lr: 0.00001
b1: 0.9
b2: 0.95
momentum: 0.9
warmup: 100 # warmup required for AdamW

micro_batch: 8
forget_accum_steps: 5
kl_accum_steps: 5
eval_accum_steps: 5
eval_every: 99

epochs: 3
steps: 100
clip: 1.0

alpha: 0.05
beta: 1.0
delta: 0
damp: 0

# referred to losses
tol: 0.02
beta_npo: 0.1
